--- 
layout: post
title: Robot Jenga Assistant
intro: ROS2, Computer Vision, Machine Learning
---

In this project I worked with a group using ROS2, computer vision, and machine learning to make a robotic jenga assistant. I mainly worked on the computer vision aspects of this 
project. Our main goal was to make it so that when there was a brick partially pushed out of the jenga tower the robot would be able to find the brick, move to it,
grab it, remove it from the tower, place it on top in the right orientation, then do it again.

The first step in setting up the robot was to create a calibration method so that we knew where everything was realtive to the robot. To do this we used tags and computer vision
by moving the robot to a calibration position so that the end effector was in view of the camera then putting an april tag in the end effector. This meant that the camera was able to create a frame at the center of the april tag that corresponded to the frame of the end effector. Since we knew the realtion between the camer and the april tag and the base of the robot and the end effector we were able to create a static transform between the camera and the base of the robot once the tag was detected. Adding this frame meant that anything that was within the camera's frame would be able to be converted into the frame of the robot so we would be able to accurately move the end effector to desired positions. The image below shows our robot in its calibration position while holding the april tag.

After calibrating the program entered an initial scanning mode where the program scanned through each layer of the depth map generated by our realsesnse camera until it located the top of the tower. Once it found the top of the tower it saved the depth information, the coordinates of the centroid, and used openCV's line detection to record the starting orientation of the top of the tower. It determined the top of the tower by finding a contour larger than a specified size and detect the orientation of the top of the tower using canny edge detection then houghlines on the image of the lines in a masked area. An example of what the line dection looks like is shown below. After finding the top of the tower the program would continue to scan until it detected another large surface area and it would save that depth as the table. This scanning process would give us a depth range from the top of the tower to the table where we were looking for removed blocks. 

